\documentclass[letterpaper,12pt,oneside]{article}
\usepackage[paperwidth=8.5in,paperheight=11in,top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{setspace}
\usepackage[colorlinks=true,allcolors=Blue]{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{indentfirst}
\usepackage{titlesec}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{outlines}
\usepackage{lineno}
\usepackage{array}
\usepackage{times}
\usepackage{cleveref}
\usepackage{acronym}
\usepackage[position=t]{subfig}
\usepackage{paralist}
\usepackage[noae]{Sweave}
\usepackage{natbib}
\usepackage{array}
\usepackage{pdflscape}
\usepackage{bm}
\usepackage{showlabels}
\usepackage{outlines}
\bibpunct{(}{)}{,}{a}{}{,}

% page margins and section title formatting
\linespread{1.5}
\setlength{\footskip}{0.5in}
\titleformat*{\section}{\Large\bf\em}
\titleformat*{\subsection}{\singlespace\large\bf}
\titleformat*{\subsubsection}{\singlespace\normalsize\bf\em}
\titlespacing{\section}{0in}{0in}{0in}
\titlespacing{\subsection}{0in}{0in}{0in}
\titlespacing{\subsubsection}{0in}{0in}{0in}

% cleveref options
\crefname{table}{Table}{Tables}
\crefname{figure}{Fig.}{Figs.}
\renewcommand{\figurename}{Fig.}

% aliased citations
% \defcitealias{FLDEP12}{FLDEP 2012}

%acronyms
\acrodef{GAM}{generalized additive models}
\acrodef{WRTDS}{weighted regression on time, discharge, and season}

%knitr options
<<setup,include=F,cache=F>>=
library(knitr)
opts_chunk$set(fig.path = 'figs/', fig.align = 'center', fig.show = 'hold', message = F, echo = F, results = 'asis', dev = 'pdf', dev.args = list(family = 'serif'), fig.pos = '!ht', warning = F)
options(replace.assign = TRUE, width = 90, digits = 1)
@

% get the version based on commit date
<<echo = FALSE, cache = FALSE>>=
raw <- system('git log -1', intern = TRUE)
raw <- raw[grep('^Date', raw)]
raw <- paste('Version', raw)
@

% R libs
<<echo = FALSE>>=
library(httr)
library(ggplot2)
library(dplyr)
library(scales)
library(wesanderson)
@

% get online bib file
<<echo = FALSE, cache = FALSE>>=
refs <- GET('https://raw.githubusercontent.com/fawda123/refs/master/refs.bib')
refs <- rawToChar(refs$content)
writeLines(refs, con = file('refs.bib'))
@

\begin{document}

\raggedbottom
% \linenumbers
\raggedright
\urlstyle{same}
\setlength{\parindent}{0.5in}
\renewcommand\refname{References \vspace{12pt}}

\begin{singlespace}
\title{{\bf {\Large Comparison of weighted regression and additive models for trend evaluation of water quality in tidal waters}}}
\author{
  {\bf {\normalsize Marcus W. Beck$^1$, Rebecca Murphy$^2$}}
  \\\\{\textit {\normalsize $^1$ORISE Research Participation Program}}
  \\{\textit {\normalsize USEPA National Health and Environmental Effects Research Laboratory}}
  \\{\textit {\normalsize Gulf Ecology Division, 1 Sabine Island Drive, Gulf Breeze, FL 32561}}
	\\{\textit {\normalsize Phone: 850-934-2480, Fax: 850-934-2401, Email: \href{mailto:beck.marcus@epa.gov}{beck.marcus@epa.gov}}}
  \\\\{\textit {\normalsize $^2$UMCES at Chesapeake Bay Program}}
	\\{\textit {\normalsize 410 Severn Avenue, Suite 112, Annapolis, MD 21403}}
	\\{\textit {\normalsize Phone: 410-267-9837, Fax: 410-267-5777, Email: \href{mailto:rmurphy@chesapeakbay.net}{rmurphy@chesapeakebay.net}}}
  \vspace{1in} 
  \\ \Sexpr{raw}
	}
\date{}
\maketitle
\end{singlespace}
\clearpage

\section*{Abstract}

\noindent \textit{Key words}:

\clearpage

\acresetall

\section{Introduction}

\begin{outline}
\0 Needs
\1 Quantitative tools that describe trends in water quality time series are needed to identify factors that influence ecosystem condition and to evaluate the effects of management activities in the context of multiple drivers
\1 Recent adaptation of statistical models for evaluating water quality time series have shown promise for application in tidal waters, specifically \ac{GAM} and \ac{WRTDS}
\1 These similar techniques can be used to quantify relationships between response measures and different drivers that may vary over time, in addition to an evaluation of trends independent of variation in freshwater inputs
\1 The relative merits of each approach have not been evaluated, particularly related to accuracy of the empirical description and the desired products for trend evaluation
\1 Such a comparison could inform the use of each model for addressing management or restoration needs or for developing more robust descriptions of long-term changes in ecosystem characteristics
\0 Goal: Provide a description of the relative abilities of \acp{GAM} and \ac{WRTDS} to describe long-term changes in time series of response endpoints in tidal waters
\0 Objectives:
\1 Provide a narrative comparison of the statistical foundation of each technique, both as a general description and as a means to evaluate water quality time series
\1 Use each technique to develop an empirical description of water quality changes in a common dataset with known historical changes in water quality drivers
\1 Apply the models to simulated data to evaluate ability of the models to describe true changes
\1 Compare each technique's ability to describe changes, as well as the differences in the information provided by each
\1 Provide recommendations on the most appropriate context for using each method
\end{outline}

\section{Methods}

\subsection{Study site}

The Patuxent River Estuary... \\
Observed trends over time \\
longitudinal gradient from watershed to mainstem influences, LE1.2, TF1.6\\
Show plots of trends over time in observed data \\

\subsection{Model descriptions}

How, Similarities, differences, optimal smoothing

The selection of optimal model parameters is a challenge that represents a tradeoff between model precision and ability to generalize to novel datasets.  Weighted regression requires identifying optimal half-window widths, whereas \ac{GAM} requires identifying the optimal degrees of freedom for the smoothing parameter.  Overfitting a model with excessively small window widths or too many degrees of freedom will minimize prediction error but prevent extrapolation of results to different datasets. Similarly, underfitting a model with large window widths or very few degrees of freedom will reduce precision but will improve the ability to generalize results to a different dataset. From a statistical perspective, the optimal smoothing provides a balance between over- and under-fitting.  Both models use a form of cross-validation to identify model parameters that maximize the precision of model predictions with a novel dataset.   

The basic premise of cross-validation is to identify the optimal set of model parameters that minimize prediction error on a dataset that was not used to develop the model.  For \acp{GAM} \citep{Hastie90,Zuur12}... Similarly, the tidal adaptation of \ac{WRTDS} used k-fold cross-validation to identify the optimal model parameters.  The dataset was separated into ten disjoint sets, such that ten models were evaluated for every combination of k - 1 training and remaining test datasets. That is, the training dataset for each fold was all k - 1 folds and the test dataset was the remaining fold, repeated k times. The average prediction error of the training datasets across k folds provides an indication of model performance for the given combination of half-window widths.  The optimum window widths were those that provided minimum errors on the test data.  Evaluating multiple combinations of window-widths can be computationally intensive. An optimization function was used to more efficiently evaluate model parameters using a search algorithm.  Window widths were searched using the limited-memory modification of the BFGS quasi-Newton method that imposes upper and lower bounds for each parameter \citep{Byrd95}.  The chosen parameters were based on a selected convergence tolerance for the error minimization of the search algorithm.  

\subsection{Comparison of modelled trends}

Explanatory power of each method - explained variance/fit in the response, histograms of errors (see page 14 in Moyer) - we can test for significant differences in the errors using a two-sided t-test.  Also see page 24/25 in Moyer for average difference comparisons between methods. \\
Similarity of predictions - observed data, simple scatterplots, similarity coefficients, similarity by time periods, etc.\\
Indications of change - direction/magnitude of trends by different time periods

\subsection{Comparison of flow-normalized trends}

Simulations - used flow record from USGS xxxx station and daily chlorophyll record from xxxx.  The basic approach was to create simulated daily time series with the same statistical properties as actual data, then sample the daily time series at monthly and bimonthly time steps to evaluate certainty of WRTDS and GAMs to identify flow-normalized trends.  Flow data were simulated by first creating a stational seasonal regression of flow over time.  The residuals from this regression were used to estimate the error distribution using an auto-regressive, moving average model.  Results of this model were used to generate random errors from a standard normal distribution.  The random, serially-correlated errors were multiplied by the standard deviation of the residuals, then added to the seasonal component of original model to create a simulated, daily log-flow time series.  The chlorophyll time series was created using a similar approach with minor differences.  The first step was to estimate the error component of the chlorophyll time series by fitting a \ac{WRTDS} model using one year of data from the whole time series.  This approach was used rather than a simple seasonal model to remove any confounding effect of flow on the error structure.  The error distribution was estimated from the residuals as before.  Standard error estimates from the regression used at each point in the one-year time series were also retained for each residual.  Random errors using the estimated auto-regressive structures were simulated for the entire year and multiplied by the corresponding standard error estimate from the regression.  The entire year was repeated for every year in the observed time series.  All simulated errors were rescaled to the range of the original residuals that were used to estimate the distribution.  

The simulated chlorophyll time series was then created by estimating the seasonal component from the observed time series, with the assumption that this component represented a flow-independent time series.  The chlorophyll error time series was then added to the seasonal model.  Finally, the simulated flow-component was added to the simulated chlorophyll time series to create a combined chlorophyll-flow time series.  The flow-component was first centered at zero and multiplied by a vector of coefficient that represented the relative effect of flow through the time series.  

Similarity of flow-normalized results - simulated data, simple scatterplots, similarity coefficients, similarity by time periods, etc.

\section{Results}

\begin{outline}
\0 Predicions with actual data
\0 Simulations
\end{outline}

\section{Discussion}

\begin{outline}
\0 Qualitative comparison
\1 Computational requirements and potential limitations
\1 Data needs or transferability of each technique to novel datasets
\1 Products, e.g., conditional quantiles of \ac{WRTDS}, confidence intervals for \acp{GAM}, handling censored data, hypothesis testing vs description
\1 Appropriate context for using each approach
\end{outline}

\subsection{Conclusions}

%%%%%%
% refs
\clearpage
\begin{singlespace}
\bibliographystyle{apalike_mine}
\bibliography{refs}
\end{singlespace}
\clearpage

%%%%%%
% tables

%%%%%%
% figures
%%%%%%
<<eval = F, echo = F, cache = F>>=
load(file = 'data/pax_data.RData')
load(file = 'data/pax_meta.RData')
load(file = 'data/pax_clip.RData')

# color palette
cols <- wes_palette('Zissou', 100, 'continuous') %>% 
  as.character %>% 
  .[1:60]

# change default ggplot theme
theme_mine <- function (base_size = 12, base_family = "") {
  theme_bw(base_size = base_size, base_family = base_family) %+replace% 
  theme(
    axis.text.x = element_text(size = 8), 
    plot.background = element_rect(fill='transparent', 
      colour = NA),
    panel.background = element_rect(fill='transparent', 
      colour = NA),
    legend.background = element_rect(fill='transparent', 
      colour = NA),
    strip.background = element_rect(fill = 
        alpha(cols[length(cols)], 0.5)),
    legend.key = element_rect(fill = 'transparent', 
      colour = NA)
    )   
}
theme_set(theme_mine())

# y axis label
y_lab <- expression(
  paste('ln-chlorophyll  ',italic(a),' (',italic(mu),'g',l^-1,')')
  )

# color for water for continuity with last fig
wt_col <- wes_palette('Zissou', 100, 'continuous')[10] %>% 
  alpha(0.8)

# function for color that is not linear by rank
cRamp <- function(x, alpha_val = 0.8){

  # colors to use
  cols <- wes_palette('Zissou', 1000, 'continuous') %>% 
    as.character %>% 
    .[1:600]

  # map the values to the colors
  x_rang <- (x - min(x, na.rm = T))/diff(range(x, na.rm = T))
  cols <- colorRamp(cols)(x_rang)
  apply(cols, 1, function(val){
    if(NaN %in% val) NA
    else
    rgb(val[1], val[2], val[3], alpha = alpha_val * 255, maxColorValue = 255)
    })
  
  }

# format pax_meta for mrg with pax_plo
pax_meta <- select(pax_meta, STATION, LONG, LAT) %>% 
  mutate(STATION = as.character(STATION))

# add month to pax data
pax_data <- mutate(pax_data, 
  mo = as.numeric(strftime(date, '%m')),
  yr = strftime(date, '%Y')
  ) %>% 
  mutate(STATION = as.character(STATION))

# get annual medians by station, add coords
pax_yr <- group_by(pax_data, STATION, yr) %>% 
  summarize(lnchla = median(lnchla, na.rm = T)) %>% 
  mutate(
    ptsz = rescale(lnchla, c(3, 12)),
    cols = cRamp(lnchla, alpha_val = 1)
    ) %>% 
  ungroup %>% 
  left_join(., pax_meta, by = 'STATION')

# do this with facet somehow...
ggplot(pax_meta, aes(x = LONG, y = LAT)) + 
  geom_polygon(data = pax_clip, aes(x = long, y = lat, group = group),
    fill = wt_col) +
  coord_map(
    xlim = c(-76.78, -76.36),
    ylim = c(38.27, 38.85)
  ) +
  # geom_text(data = yr_labs, aes(label = yr)) + 
  geom_point(data = pax_yr, aes(group = yr, size = lnchla, fill = lnchla), 
    pch = 21) +
  facet_wrap(~yr) +
  theme_mine() + 
  scale_fill_gradientn(colours = cols) +
  theme(axis.title = element_blank())

# grid.arrange(out_yrs[[1]], out_yrs[[2]], out_yrs[[3]], out_yrs[[4]], out_yrs[[5]], out_yrs[[6]], out_yrs[[7]], out_yrs[[8]], out_yrs[[9]], out_yrs[[10]], out_yrs[[11]], out_yrs[[12]], out_yrs[[13]], out_yrs[[14]], out_yrs[[15]], out_yrs[[16]], out_yrs[[17]], out_yrs[[18]], out_yrs[[19]], out_yrs[[20]], out_yrs[[21]], out_yrs[[22]], out_yrs[[23]], out_yrs[[24]], out_yrs[[25]], out_yrs[[26]], out_yrs[[27]], out_yrs[[28]], out_yrs[[29]], ncol = 3)

@


\end{document}